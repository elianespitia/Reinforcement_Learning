{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6694cc",
   "metadata": {},
   "source": [
    "# **01_exploration.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5f2cd",
   "metadata": {},
   "source": [
    "### **Preparamos el entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab7976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60c426",
   "metadata": {},
   "source": [
    "### **Creamos el ambiente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a463797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elian/Documentos/Reinforcement_Learning/Project_1_cross_entropy_method_on_CartPole/venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, termitated, truncated, info = env.step(action)\n",
    "    done = termitated or truncated\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2ec85",
   "metadata": {},
   "source": [
    "### **Vemos si Tenemos una GPU Disponible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66de2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando Dispositivo:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando Dispositivo: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f5831",
   "metadata": {},
   "source": [
    "### **Definimos la red neuronal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6618d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b197a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(obs_size, n_actions).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d14861",
   "metadata": {},
   "source": [
    "### **Función para Generar Datos de los Episodios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4159480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy_net, device):\n",
    "    obs, _ = env.reset()\n",
    "    obs_list = []\n",
    "    action_list = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        obs_v = torch.tensor([obs], dtype=torch.float32).to(device)\n",
    "        action_probs = torch.softmax(policy_net(obs_v), dim=1)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        obs_list.append(obs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        obs, reward, terminated, trucated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or trucated\n",
    "    \n",
    "    return total_reward, obs_list, action_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8364c77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: 16.0\n",
      "\n",
      "obs_list: [array([-0.03007691,  0.01168857,  0.01957042, -0.02855192], dtype=float32), array([-0.02984314, -0.18370849,  0.01899938,  0.27024087], dtype=float32), array([-0.03351731, -0.37909633,  0.02440419,  0.5688552 ], dtype=float32), array([-0.04109924, -0.18432502,  0.0357813 ,  0.28395936], dtype=float32), array([-0.04478574,  0.01026881,  0.04146048,  0.00277293], dtype=float32), array([-0.04458036,  0.20477238,  0.04151595, -0.27654582], dtype=float32), array([-0.04048492,  0.3992782 ,  0.03598503, -0.555851  ], dtype=float32), array([-0.03249935,  0.20366998,  0.02486801, -0.2520513 ], dtype=float32), array([-0.02842595,  0.39842817,  0.01982698, -0.5367878 ], dtype=float32), array([-0.02045739,  0.59326583,  0.00909123, -0.8231581 ], dtype=float32), array([-0.00859207,  0.78826225, -0.00737193, -1.1129678 ], dtype=float32), array([ 0.00717317,  0.9834802 , -0.02963129, -1.4079542 ], dtype=float32), array([ 0.02684278,  1.178957  , -0.05779038, -1.709751  ], dtype=float32), array([ 0.05042192,  1.3746934 , -0.0919854 , -2.0198464 ], dtype=float32), array([ 0.07791579,  1.5706406 , -0.13238232, -2.33953   ], dtype=float32), array([ 0.1093286 ,  1.3769372 , -0.17917292, -2.0903242 ], dtype=float32)]\n",
      "\n",
      "action_list: [0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25359/184973026.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  obs_v = torch.tensor([obs], dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo del agente jugando con la red neuronal sin entrenar\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "total_reward, obs_list, action_list = generate_episode(env, policy_net, device)\n",
    "\n",
    "print(\"total_reward:\", total_reward)\n",
    "print(\"\\nobs_list:\", obs_list)\n",
    "print(\"\\naction_list:\", action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f159865",
   "metadata": {},
   "source": [
    "### **Generar Múltiples Episodios y Seleccionar los Mejores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93625d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env, policy_net, device, batch_size, percentile=70):\n",
    "    rewards = []\n",
    "    batch_obs = []\n",
    "    batch_actions = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        total_reward, obs_list, action_list = generate_episode(env, policy_net, device)\n",
    "        rewards.append(total_reward)\n",
    "        batch_obs.append(obs_list)\n",
    "        batch_actions.append(action_list)\n",
    "        \n",
    "    reward_threshold = np.percentile(rewards, percentile)\n",
    "    \n",
    "    train_obs = []\n",
    "    train_actions = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if rewards[i] >= reward_threshold:\n",
    "            train_obs.extend(batch_obs[i])\n",
    "            train_actions.extend(batch_actions[i])\n",
    "            \n",
    "    train_obs = torch.tensor(train_obs, dtype=torch.float32).to(device)\n",
    "    train_actions = torch.tensor(train_actions, dtype=torch.long).to(device)        \n",
    "    \n",
    "    return train_obs, train_actions, np.mean(rewards), reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacb658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_obs: tensor([[ 0.0276,  0.0492,  0.0194, -0.0081],\n",
      "        [ 0.0286,  0.2441,  0.0192, -0.2946],\n",
      "        [ 0.0335,  0.0487,  0.0134,  0.0041],\n",
      "        ...,\n",
      "        [ 0.1215,  1.1329, -0.1277, -1.5074],\n",
      "        [ 0.1441,  0.9395, -0.1578, -1.2572],\n",
      "        [ 0.1629,  1.1363, -0.1829, -1.5948]])\n",
      "\n",
      "train_actions: tensor([1, 0, 1,  ..., 0, 1, 0])\n",
      "\n",
      "avg_reward: 23.92\n",
      "\n",
      "threshold: 28.299999999999997\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo degeneración de un batch con la red neuronal sin entrenar\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "train_o, train_a, avg_rew, threshold = generate_batch(env, policy_net, device, batch_size=100)\n",
    "\n",
    "print(\"train_obs:\", train_o)\n",
    "print(\"\\ntrain_actions:\", train_a)\n",
    "print(\"\\navg_reward:\", avg_rew)\n",
    "print(\"\\nthreshold:\", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e24dff",
   "metadata": {},
   "source": [
    "### **Entrenamos la Red con los Mejores Episodios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e5604fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(policy_net, optimizer, train_obs, train_actions):\n",
    "    optimizer.zero_grad()\n",
    "    action_scores = policy_net(train_obs)\n",
    "    loss = nn.CrossEntropyLoss()(action_scores, train_actions)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c34171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6802474856376648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(policy_net, optimizer, train_o, train_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d359e",
   "metadata": {},
   "source": [
    "### **Entrenamiento Iterativo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13468688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: loss=0.682, mean_reward=30.2, reward_threshold=32.0\n",
      "Iter 1: loss=0.670, mean_reward=30.8, reward_threshold=36.0\n",
      "Iter 2: loss=0.658, mean_reward=32.1, reward_threshold=36.3\n",
      "Iter 3: loss=0.652, mean_reward=30.5, reward_threshold=36.0\n",
      "Iter 4: loss=0.640, mean_reward=32.9, reward_threshold=36.0\n",
      "Iter 5: loss=0.624, mean_reward=51.1, reward_threshold=57.6\n",
      "Iter 6: loss=0.607, mean_reward=50.4, reward_threshold=57.2\n",
      "Iter 7: loss=0.607, mean_reward=60.0, reward_threshold=62.6\n",
      "Iter 8: loss=0.598, mean_reward=53.2, reward_threshold=54.2\n",
      "Iter 9: loss=0.592, mean_reward=71.5, reward_threshold=72.0\n",
      "Iter 10: loss=0.585, mean_reward=60.1, reward_threshold=69.3\n",
      "Iter 11: loss=0.580, mean_reward=68.7, reward_threshold=74.0\n",
      "Iter 12: loss=0.577, mean_reward=89.1, reward_threshold=95.8\n",
      "Iter 13: loss=0.569, mean_reward=100.7, reward_threshold=99.6\n",
      "Iter 14: loss=0.565, mean_reward=107.9, reward_threshold=119.2\n",
      "Iter 15: loss=0.557, mean_reward=141.1, reward_threshold=164.9\n",
      "Iter 16: loss=0.556, mean_reward=160.9, reward_threshold=175.9\n",
      "Iter 17: loss=0.551, mean_reward=160.5, reward_threshold=165.9\n",
      "Iter 18: loss=0.553, mean_reward=182.3, reward_threshold=224.8\n",
      "Iter 19: loss=0.551, mean_reward=197.9, reward_threshold=239.2\n",
      "Iter 20: loss=0.548, mean_reward=198.7, reward_threshold=213.9\n",
      "Iter 21: loss=0.544, mean_reward=199.5, reward_threshold=243.6\n",
      "Iter 22: loss=0.534, mean_reward=213.8, reward_threshold=260.6\n",
      "Iter 23: loss=0.533, mean_reward=266.1, reward_threshold=322.2\n",
      "Iter 24: loss=0.534, mean_reward=246.5, reward_threshold=301.2\n",
      "Iter 25: loss=0.529, mean_reward=297.3, reward_threshold=347.7\n",
      "Iter 26: loss=0.530, mean_reward=325.6, reward_threshold=382.3\n",
      "Iter 27: loss=0.533, mean_reward=304.6, reward_threshold=340.7\n",
      "Iter 28: loss=0.527, mean_reward=290.6, reward_threshold=311.6\n",
      "Iter 29: loss=0.535, mean_reward=325.4, reward_threshold=369.3\n",
      "Iter 30: loss=0.524, mean_reward=391.9, reward_threshold=500.0\n",
      "Iter 31: loss=0.517, mean_reward=434.0, reward_threshold=500.0\n",
      "Iter 32: loss=0.510, mean_reward=456.2, reward_threshold=500.0\n",
      "Iter 33: loss=0.517, mean_reward=477.1, reward_threshold=500.0\n",
      "Iter 34: loss=0.516, mean_reward=471.0, reward_threshold=500.0\n",
      "Iter 35: loss=0.522, mean_reward=468.3, reward_threshold=500.0\n",
      "Iter 36: loss=0.522, mean_reward=455.0, reward_threshold=500.0\n",
      "Iter 37: loss=0.516, mean_reward=489.3, reward_threshold=500.0\n",
      "Iter 38: loss=0.515, mean_reward=479.3, reward_threshold=500.0\n",
      "Iter 39: loss=0.516, mean_reward=463.0, reward_threshold=500.0\n",
      "Iter 40: loss=0.513, mean_reward=478.8, reward_threshold=500.0\n",
      "Iter 41: loss=0.516, mean_reward=475.5, reward_threshold=500.0\n",
      "Iter 42: loss=0.511, mean_reward=480.9, reward_threshold=500.0\n",
      "Iter 43: loss=0.515, mean_reward=493.6, reward_threshold=500.0\n",
      "Iter 44: loss=0.507, mean_reward=491.3, reward_threshold=500.0\n",
      "Iter 45: loss=0.507, mean_reward=485.9, reward_threshold=500.0\n",
      "Iter 46: loss=0.505, mean_reward=488.0, reward_threshold=500.0\n",
      "Iter 47: loss=0.506, mean_reward=497.2, reward_threshold=500.0\n",
      "Iter 48: loss=0.502, mean_reward=492.2, reward_threshold=500.0\n",
      "Iter 49: loss=0.507, mean_reward=486.1, reward_threshold=500.0\n",
      "\n",
      "Entrenamiento completado en 155.36 segundos\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "n_iterations = 50\n",
    "batch_size = 50\n",
    "percentile = 70\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    train_obs, train_actions, reward_mean, reward_threshold = generate_batch(env, policy_net, device, batch_size, percentile)\n",
    "    loss = train_step(policy_net, optimizer, train_obs, train_actions)\n",
    "    print(f\"Iter {i}: loss={loss:.3f}, mean_reward={reward_mean:.1f}, reward_threshold={reward_threshold:.1f}\")\n",
    "    \n",
    "end_time = time.time() \n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nEntrenamiento completado en {elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe819f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_trainde_agent(env, policy_net, n_episodes):\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs_v = torch.tensor([obs], dtype=torch.float32).to(device)\n",
    "            action_probs = torch.softmax(policy_net(obs_v), dim=1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        print(f\"Episode {episode+1}: total_reward={total_reward}\")\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e871fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: total_reward=469.0\n",
      "Episode 2: total_reward=500.0\n",
      "Episode 3: total_reward=500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "play_trainde_agent(env, policy_net, n_episodes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project_1_kernel)",
   "language": "python",
   "name": "project_1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
